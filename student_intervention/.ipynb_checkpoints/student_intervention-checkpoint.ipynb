{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Supervised Learning\n",
    "## Project: Building a Student Intervention System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second project of the Machine Learning Engineer Nanodegree! In this notebook, some template code has already been provided for you, and it will be your job to implement the additional functionality necessary to successfully complete this project. Sections that begin with **'Implementation'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a `'TODO'` statement. Please be sure to read the instructions carefully!\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.  \n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Classification vs. Regression\n",
    "*Your goal for this project is to identify students who might need early intervention before they fail to graduate. Which type of supervised learning problem is this, classification or regression? Why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: ** This is a typical calssification problem, because we are tring to predict based on many features whether a student is likely to be \"passed\" or \"failed\" toward graduation, and the output we aim for is a discrete variable of \"passed\" or \"failed\". If the student is classified as \"failed\", he or she will need early intervention before he/she fails to graduate.\n",
    "\n",
    "And we do this by training a model using historical student data record where each student is labeled as \"passed\" or failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "Run the code cell below to load necessary Python libraries and load the student data. Note that the last column from this dataset, `'passed'`, will be our target label (whether the student graduated or didn't graduate). All other columns are features about each student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Read student data\n",
    "student_data = pd.read_csv(\"student-data.csv\")\n",
    "print(\"Student data read successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Data Exploration\n",
    "Let's begin by investigating the dataset to determine how many students we have information on, and learn about the graduation rate among these students. In the code cell below, you will need to compute the following:\n",
    "- The total number of students, `n_students`.\n",
    "- The total number of features for each student, `n_features`.\n",
    "- The number of those students who passed, `n_passed`.\n",
    "- The number of those students who failed, `n_failed`.\n",
    "- The graduation rate of the class, `grad_rate`, in percent (%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
      "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
      "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
      "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
      "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
      "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
      "\n",
      "   ...   internet romantic  famrel  freetime  goout Dalc Walc health absences  \\\n",
      "0  ...         no       no       4         3      4    1    1      3        6   \n",
      "1  ...        yes       no       5         3      3    1    1      3        4   \n",
      "2  ...        yes       no       4         3      2    2    3      3       10   \n",
      "3  ...        yes      yes       3         2      2    1    1      5        2   \n",
      "4  ...         no       no       4         3      2    1    2      5        4   \n",
      "\n",
      "  passed  \n",
      "0     no  \n",
      "1     no  \n",
      "2    yes  \n",
      "3    yes  \n",
      "4    yes  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "Index(['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',\n",
      "       'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime',\n",
      "       'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery',\n",
      "       'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc',\n",
      "       'Walc', 'health', 'absences', 'passed'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.696203</td>\n",
       "      <td>2.749367</td>\n",
       "      <td>2.521519</td>\n",
       "      <td>1.448101</td>\n",
       "      <td>2.035443</td>\n",
       "      <td>0.334177</td>\n",
       "      <td>3.944304</td>\n",
       "      <td>3.235443</td>\n",
       "      <td>3.108861</td>\n",
       "      <td>1.481013</td>\n",
       "      <td>2.291139</td>\n",
       "      <td>3.554430</td>\n",
       "      <td>5.708861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.276043</td>\n",
       "      <td>1.094735</td>\n",
       "      <td>1.088201</td>\n",
       "      <td>0.697505</td>\n",
       "      <td>0.839240</td>\n",
       "      <td>0.743651</td>\n",
       "      <td>0.896659</td>\n",
       "      <td>0.998862</td>\n",
       "      <td>1.113278</td>\n",
       "      <td>0.890741</td>\n",
       "      <td>1.287897</td>\n",
       "      <td>1.390303</td>\n",
       "      <td>8.003096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age        Medu        Fedu  traveltime   studytime    failures  \\\n",
       "count  395.000000  395.000000  395.000000  395.000000  395.000000  395.000000   \n",
       "mean    16.696203    2.749367    2.521519    1.448101    2.035443    0.334177   \n",
       "std      1.276043    1.094735    1.088201    0.697505    0.839240    0.743651   \n",
       "min     15.000000    0.000000    0.000000    1.000000    1.000000    0.000000   \n",
       "25%     16.000000    2.000000    2.000000    1.000000    1.000000    0.000000   \n",
       "50%     17.000000    3.000000    2.000000    1.000000    2.000000    0.000000   \n",
       "75%     18.000000    4.000000    3.000000    2.000000    2.000000    0.000000   \n",
       "max     22.000000    4.000000    4.000000    4.000000    4.000000    3.000000   \n",
       "\n",
       "           famrel    freetime       goout        Dalc        Walc      health  \\\n",
       "count  395.000000  395.000000  395.000000  395.000000  395.000000  395.000000   \n",
       "mean     3.944304    3.235443    3.108861    1.481013    2.291139    3.554430   \n",
       "std      0.896659    0.998862    1.113278    0.890741    1.287897    1.390303   \n",
       "min      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "25%      4.000000    3.000000    2.000000    1.000000    1.000000    3.000000   \n",
       "50%      4.000000    3.000000    3.000000    1.000000    2.000000    4.000000   \n",
       "75%      5.000000    4.000000    4.000000    2.000000    3.000000    5.000000   \n",
       "max      5.000000    5.000000    5.000000    5.000000    5.000000    5.000000   \n",
       "\n",
       "         absences  \n",
       "count  395.000000  \n",
       "mean     5.708861  \n",
       "std      8.003096  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      4.000000  \n",
       "75%      8.000000  \n",
       "max     75.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(student_data.head())\n",
    "print(student_data.columns)\n",
    "student_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of students: 395\n",
      "Number of features: 30\n",
      "Number of students who passed: 265\n",
      "Number of students who failed: 130\n",
      "Graduation rate of the class: 67.09%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Calculate number of students\n",
    "n_students = student_data.shape[0]\n",
    "\n",
    "# TODO: Calculate number of features\n",
    "n_features = student_data.shape[1]-1\n",
    "\n",
    "# TODO: Calculate passing students\n",
    "n_passed = len(student_data[student_data['passed']==\"yes\"])\n",
    "\n",
    "# TODO: Calculate failing students\n",
    "n_failed = len(student_data[student_data['passed']==\"no\"])\n",
    "\n",
    "# TODO: Calculate graduation rate\n",
    "grad_rate = n_passed / n_students * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"Total number of students: {}\".format(n_students))\n",
    "print(\"Number of features: {}\".format(n_features))\n",
    "print(\"Number of students who passed: {}\".format(n_passed))\n",
    "print(\"Number of students who failed: {}\".format(n_failed))\n",
    "print(\"Graduation rate of the class: {:.2f}%\".format(grad_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "In this section, we will prepare the data for modeling, training and testing.\n",
    "\n",
    "### Identify feature and target columns\n",
    "It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.\n",
    "\n",
    "Run the code cell below to separate the student data into feature and target columns to see if any features are non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns:\n",
      "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
      "\n",
      "Target column: passed\n",
      "\n",
      "Feature values:\n",
      "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
      "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
      "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
      "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
      "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
      "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
      "\n",
      "    ...    higher internet  romantic  famrel  freetime goout Dalc Walc health  \\\n",
      "0   ...       yes       no        no       4         3     4    1    1      3   \n",
      "1   ...       yes      yes        no       5         3     3    1    1      3   \n",
      "2   ...       yes      yes        no       4         3     2    2    3      3   \n",
      "3   ...       yes      yes       yes       3         2     2    1    1      5   \n",
      "4   ...       yes       no        no       4         3     2    1    2      5   \n",
      "\n",
      "  absences  \n",
      "0        6  \n",
      "1        4  \n",
      "2       10  \n",
      "3        2  \n",
      "4        4  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract feature columns\n",
    "feature_cols = list(student_data.columns[:-1])\n",
    "\n",
    "# Extract target column 'passed'\n",
    "target_col = student_data.columns[-1] \n",
    "\n",
    "# Show the list of columns\n",
    "print(\"Feature columns:\\n{}\".format(feature_cols))\n",
    "print(\"\\nTarget column: {}\".format(target_col))\n",
    "\n",
    "# Separate the data into feature data and target data (X_all and y_all, respectively)\n",
    "X_all = student_data[feature_cols]\n",
    "y_all = student_data[target_col]\n",
    "\n",
    "# Show the feature information by printing the first five rows\n",
    "print(\"\\nFeature values:\")\n",
    "print(X_all.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Feature Columns\n",
    "\n",
    "As you can see, there are several non-numeric columns that need to be converted! Many of them are simply `yes`/`no`, e.g. `internet`. These can be reasonably converted into `1`/`0` (binary) values.\n",
    "\n",
    "Other columns, like `Mjob` and `Fjob`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `Fjob_teacher`, `Fjob_other`, `Fjob_services`, etc.), and assign a `1` to one of them and `0` to all others.\n",
    "\n",
    "These generated columns are sometimes called _dummy variables_, and we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to perform this transformation. Run the code cell below to perform the preprocessing routine discussed in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (48 total features):\n",
      "['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
      "[dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('int64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('int64'), dtype('int64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64')]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_features(X):\n",
    "    ''' Preprocesses the student data and converts non-numeric binary variables into\n",
    "        binary (0/1) variables. Converts categorical variables into dummy variables. '''\n",
    "    \n",
    "    # Initialize new output DataFrame\n",
    "    output = pd.DataFrame(index = X.index)\n",
    "\n",
    "    # Investigate each feature column for the data\n",
    "    for col, col_data in X.iteritems():\n",
    "        \n",
    "        # If data type is non-numeric, replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "\n",
    "        # If data type is categorical, convert to dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            # Example: 'school' => 'school_GP' and 'school_MS'\n",
    "            col_data = pd.get_dummies(col_data, prefix = col)  \n",
    "        \n",
    "        # Collect the revised columns\n",
    "        output = output.join(col_data)\n",
    "    \n",
    "    return output\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "print(\"Processed feature columns ({} total features):\\n{}\".format(len(X_all.columns), list(X_all.columns)))\n",
    "\n",
    "col_dtype = [X_all[col].dtype for col in X_all.columns]\n",
    "print(col_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_GP</th>\n",
       "      <th>school_MS</th>\n",
       "      <th>sex_F</th>\n",
       "      <th>sex_M</th>\n",
       "      <th>age</th>\n",
       "      <th>address_R</th>\n",
       "      <th>address_U</th>\n",
       "      <th>famsize_GT3</th>\n",
       "      <th>famsize_LE3</th>\n",
       "      <th>Pstatus_A</th>\n",
       "      <th>Pstatus_T</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob_at_home</th>\n",
       "      <th>Mjob_health</th>\n",
       "      <th>Mjob_other</th>\n",
       "      <th>Mjob_services</th>\n",
       "      <th>Mjob_teacher</th>\n",
       "      <th>Fjob_at_home</th>\n",
       "      <th>Fjob_health</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.883544</td>\n",
       "      <td>0.116456</td>\n",
       "      <td>0.526582</td>\n",
       "      <td>0.473418</td>\n",
       "      <td>16.696203</td>\n",
       "      <td>0.222785</td>\n",
       "      <td>0.777215</td>\n",
       "      <td>0.711392</td>\n",
       "      <td>0.288608</td>\n",
       "      <td>0.103797</td>\n",
       "      <td>0.896203</td>\n",
       "      <td>2.749367</td>\n",
       "      <td>2.521519</td>\n",
       "      <td>0.149367</td>\n",
       "      <td>0.086076</td>\n",
       "      <td>0.356962</td>\n",
       "      <td>0.260759</td>\n",
       "      <td>0.146835</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>0.045570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.321177</td>\n",
       "      <td>0.321177</td>\n",
       "      <td>0.499926</td>\n",
       "      <td>0.499926</td>\n",
       "      <td>1.276043</td>\n",
       "      <td>0.416643</td>\n",
       "      <td>0.416643</td>\n",
       "      <td>0.453690</td>\n",
       "      <td>0.453690</td>\n",
       "      <td>0.305384</td>\n",
       "      <td>0.305384</td>\n",
       "      <td>1.094735</td>\n",
       "      <td>1.088201</td>\n",
       "      <td>0.356902</td>\n",
       "      <td>0.280832</td>\n",
       "      <td>0.479711</td>\n",
       "      <td>0.439606</td>\n",
       "      <td>0.354391</td>\n",
       "      <td>0.219525</td>\n",
       "      <td>0.208814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        school_GP   school_MS       sex_F       sex_M         age   address_R  \\\n",
       "count  395.000000  395.000000  395.000000  395.000000  395.000000  395.000000   \n",
       "mean     0.883544    0.116456    0.526582    0.473418   16.696203    0.222785   \n",
       "std      0.321177    0.321177    0.499926    0.499926    1.276043    0.416643   \n",
       "min      0.000000    0.000000    0.000000    0.000000   15.000000    0.000000   \n",
       "25%      1.000000    0.000000    0.000000    0.000000   16.000000    0.000000   \n",
       "50%      1.000000    0.000000    1.000000    0.000000   17.000000    0.000000   \n",
       "75%      1.000000    0.000000    1.000000    1.000000   18.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000   22.000000    1.000000   \n",
       "\n",
       "        address_U  famsize_GT3  famsize_LE3   Pstatus_A   Pstatus_T  \\\n",
       "count  395.000000   395.000000   395.000000  395.000000  395.000000   \n",
       "mean     0.777215     0.711392     0.288608    0.103797    0.896203   \n",
       "std      0.416643     0.453690     0.453690    0.305384    0.305384   \n",
       "min      0.000000     0.000000     0.000000    0.000000    0.000000   \n",
       "25%      1.000000     0.000000     0.000000    0.000000    1.000000   \n",
       "50%      1.000000     1.000000     0.000000    0.000000    1.000000   \n",
       "75%      1.000000     1.000000     1.000000    0.000000    1.000000   \n",
       "max      1.000000     1.000000     1.000000    1.000000    1.000000   \n",
       "\n",
       "             Medu        Fedu  Mjob_at_home  Mjob_health  Mjob_other  \\\n",
       "count  395.000000  395.000000    395.000000   395.000000  395.000000   \n",
       "mean     2.749367    2.521519      0.149367     0.086076    0.356962   \n",
       "std      1.094735    1.088201      0.356902     0.280832    0.479711   \n",
       "min      0.000000    0.000000      0.000000     0.000000    0.000000   \n",
       "25%      2.000000    2.000000      0.000000     0.000000    0.000000   \n",
       "50%      3.000000    2.000000      0.000000     0.000000    0.000000   \n",
       "75%      4.000000    3.000000      0.000000     0.000000    1.000000   \n",
       "max      4.000000    4.000000      1.000000     1.000000    1.000000   \n",
       "\n",
       "       Mjob_services  Mjob_teacher  Fjob_at_home  Fjob_health  \n",
       "count     395.000000    395.000000    395.000000   395.000000  \n",
       "mean        0.260759      0.146835      0.050633     0.045570  \n",
       "std         0.439606      0.354391      0.219525     0.208814  \n",
       "min         0.000000      0.000000      0.000000     0.000000  \n",
       "25%         0.000000      0.000000      0.000000     0.000000  \n",
       "50%         0.000000      0.000000      0.000000     0.000000  \n",
       "75%         1.000000      0.000000      0.000000     0.000000  \n",
       "max         1.000000      1.000000      1.000000     1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all[X_all.columns[:20]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fjob_other</th>\n",
       "      <th>Fjob_services</th>\n",
       "      <th>Fjob_teacher</th>\n",
       "      <th>reason_course</th>\n",
       "      <th>reason_home</th>\n",
       "      <th>reason_other</th>\n",
       "      <th>reason_reputation</th>\n",
       "      <th>guardian_father</th>\n",
       "      <th>guardian_mother</th>\n",
       "      <th>guardian_other</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>schoolsup</th>\n",
       "      <th>famsup</th>\n",
       "      <th>paid</th>\n",
       "      <th>activities</th>\n",
       "      <th>nursery</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.549367</td>\n",
       "      <td>0.281013</td>\n",
       "      <td>0.073418</td>\n",
       "      <td>0.367089</td>\n",
       "      <td>0.275949</td>\n",
       "      <td>0.091139</td>\n",
       "      <td>0.265823</td>\n",
       "      <td>0.227848</td>\n",
       "      <td>0.691139</td>\n",
       "      <td>0.081013</td>\n",
       "      <td>1.448101</td>\n",
       "      <td>2.035443</td>\n",
       "      <td>0.334177</td>\n",
       "      <td>0.129114</td>\n",
       "      <td>0.612658</td>\n",
       "      <td>0.458228</td>\n",
       "      <td>0.508861</td>\n",
       "      <td>0.794937</td>\n",
       "      <td>0.949367</td>\n",
       "      <td>0.832911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.498188</td>\n",
       "      <td>0.450064</td>\n",
       "      <td>0.261152</td>\n",
       "      <td>0.482622</td>\n",
       "      <td>0.447558</td>\n",
       "      <td>0.288172</td>\n",
       "      <td>0.442331</td>\n",
       "      <td>0.419976</td>\n",
       "      <td>0.462610</td>\n",
       "      <td>0.273201</td>\n",
       "      <td>0.697505</td>\n",
       "      <td>0.839240</td>\n",
       "      <td>0.743651</td>\n",
       "      <td>0.335751</td>\n",
       "      <td>0.487761</td>\n",
       "      <td>0.498884</td>\n",
       "      <td>0.500555</td>\n",
       "      <td>0.404260</td>\n",
       "      <td>0.219525</td>\n",
       "      <td>0.373528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Fjob_other  Fjob_services  Fjob_teacher  reason_course  reason_home  \\\n",
       "count  395.000000     395.000000    395.000000     395.000000   395.000000   \n",
       "mean     0.549367       0.281013      0.073418       0.367089     0.275949   \n",
       "std      0.498188       0.450064      0.261152       0.482622     0.447558   \n",
       "min      0.000000       0.000000      0.000000       0.000000     0.000000   \n",
       "25%      0.000000       0.000000      0.000000       0.000000     0.000000   \n",
       "50%      1.000000       0.000000      0.000000       0.000000     0.000000   \n",
       "75%      1.000000       1.000000      0.000000       1.000000     1.000000   \n",
       "max      1.000000       1.000000      1.000000       1.000000     1.000000   \n",
       "\n",
       "       reason_other  reason_reputation  guardian_father  guardian_mother  \\\n",
       "count    395.000000         395.000000       395.000000       395.000000   \n",
       "mean       0.091139           0.265823         0.227848         0.691139   \n",
       "std        0.288172           0.442331         0.419976         0.462610   \n",
       "min        0.000000           0.000000         0.000000         0.000000   \n",
       "25%        0.000000           0.000000         0.000000         0.000000   \n",
       "50%        0.000000           0.000000         0.000000         1.000000   \n",
       "75%        0.000000           1.000000         0.000000         1.000000   \n",
       "max        1.000000           1.000000         1.000000         1.000000   \n",
       "\n",
       "       guardian_other  traveltime   studytime    failures   schoolsup  \\\n",
       "count      395.000000  395.000000  395.000000  395.000000  395.000000   \n",
       "mean         0.081013    1.448101    2.035443    0.334177    0.129114   \n",
       "std          0.273201    0.697505    0.839240    0.743651    0.335751   \n",
       "min          0.000000    1.000000    1.000000    0.000000    0.000000   \n",
       "25%          0.000000    1.000000    1.000000    0.000000    0.000000   \n",
       "50%          0.000000    1.000000    2.000000    0.000000    0.000000   \n",
       "75%          0.000000    2.000000    2.000000    0.000000    0.000000   \n",
       "max          1.000000    4.000000    4.000000    3.000000    1.000000   \n",
       "\n",
       "           famsup        paid  activities     nursery      higher    internet  \n",
       "count  395.000000  395.000000  395.000000  395.000000  395.000000  395.000000  \n",
       "mean     0.612658    0.458228    0.508861    0.794937    0.949367    0.832911  \n",
       "std      0.487761    0.498884    0.500555    0.404260    0.219525    0.373528  \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000    1.000000    1.000000    1.000000  \n",
       "50%      1.000000    0.000000    1.000000    1.000000    1.000000    1.000000  \n",
       "75%      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000  \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all[X_all.columns[20:40]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.334177</td>\n",
       "      <td>3.944304</td>\n",
       "      <td>3.235443</td>\n",
       "      <td>3.108861</td>\n",
       "      <td>1.481013</td>\n",
       "      <td>2.291139</td>\n",
       "      <td>3.554430</td>\n",
       "      <td>5.708861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.472300</td>\n",
       "      <td>0.896659</td>\n",
       "      <td>0.998862</td>\n",
       "      <td>1.113278</td>\n",
       "      <td>0.890741</td>\n",
       "      <td>1.287897</td>\n",
       "      <td>1.390303</td>\n",
       "      <td>8.003096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         romantic      famrel    freetime       goout        Dalc        Walc  \\\n",
       "count  395.000000  395.000000  395.000000  395.000000  395.000000  395.000000   \n",
       "mean     0.334177    3.944304    3.235443    3.108861    1.481013    2.291139   \n",
       "std      0.472300    0.896659    0.998862    1.113278    0.890741    1.287897   \n",
       "min      0.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "25%      0.000000    4.000000    3.000000    2.000000    1.000000    1.000000   \n",
       "50%      0.000000    4.000000    3.000000    3.000000    1.000000    2.000000   \n",
       "75%      1.000000    5.000000    4.000000    4.000000    2.000000    3.000000   \n",
       "max      1.000000    5.000000    5.000000    5.000000    5.000000    5.000000   \n",
       "\n",
       "           health    absences  \n",
       "count  395.000000  395.000000  \n",
       "mean     3.554430    5.708861  \n",
       "std      1.390303    8.003096  \n",
       "min      1.000000    0.000000  \n",
       "25%      3.000000    0.000000  \n",
       "50%      4.000000    4.000000  \n",
       "75%      5.000000    8.000000  \n",
       "max      5.000000   75.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all[X_all.columns[40:]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Training and Testing Data Split\n",
    "So far, we have converted all _categorical_ features into numeric values. For the next step, we split the data (both features and corresponding labels) into training and test sets. In the following code cell below, you will need to implement the following:\n",
    "- Randomly shuffle and split the data (`X_all`, `y_all`) into training and testing subsets.\n",
    "  - Use 300 training points (approximately 75%) and 95 testing points (approximately 25%).\n",
    "  - Set a `random_state` for the function(s) you use, if provided.\n",
    "  - Store the results in `X_train`, `X_test`, `y_train`, and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 300 samples.\n",
      "Testing set has 95 samples.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import any additional functionality you may need here\n",
    "import numpy.random \n",
    "numpy.random.seed(0) \n",
    "\n",
    "# TODO: Set the number of training points\n",
    "num_train = 300\n",
    "\n",
    "# Set the number of testing points\n",
    "num_test = X_all.shape[0] - num_train\n",
    "\n",
    "# TODO: Shuffle and split the dataset into the number of training and testing points above\n",
    "rand_index = np.arange(X_all.shape[0])\n",
    "numpy.random.shuffle(rand_index)\n",
    "\n",
    "X_train = X_all.iloc[rand_index[:num_train]]\n",
    "X_test = X_all.iloc[rand_index[num_train:]]\n",
    "y_train = y_all.iloc[rand_index[:num_train]]\n",
    "y_test = y_all.iloc[rand_index[num_train:]]\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing 2: standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we perform any training, it's important to standardize/ normalize the data to the standard normal distribution ~ N(0,1). \n",
    "\n",
    "Note: Only the following features needs to be normalized : age, Medu, Fedu, traveltime, studytime, failures, famrel, freetime,  goout, Dalc, Walc, health, absences.   \n",
    "\n",
    "The rest of features do NOT need to be normalized as they are binary (0,1) and normalizing them will lose the sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-1.213844e-15</td>\n",
       "      <td>-3.034610e-17</td>\n",
       "      <td>-6.439294e-17</td>\n",
       "      <td>1.665335e-16</td>\n",
       "      <td>-9.337438e-17</td>\n",
       "      <td>-1.202742e-17</td>\n",
       "      <td>-1.657933e-16</td>\n",
       "      <td>-1.139829e-16</td>\n",
       "      <td>2.294461e-17</td>\n",
       "      <td>2.235249e-16</td>\n",
       "      <td>7.179442e-17</td>\n",
       "      <td>2.412885e-16</td>\n",
       "      <td>1.480297e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.001671e+00</td>\n",
       "      <td>1.001671e+00</td>\n",
       "      <td>1.001671e+00</td>\n",
       "      <td>1.001671e+00</td>\n",
       "      <td>1.001671e+00</td>\n",
       "      <td>1.001671e+00</td>\n",
       "      <td>1.001671e+00</td>\n",
       "      <td>1.001671e+00</td>\n",
       "      <td>1.001671e+00</td>\n",
       "      <td>1.001671e+00</td>\n",
       "      <td>1.001671e+00</td>\n",
       "      <td>1.001671e+00</td>\n",
       "      <td>1.001671e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.331706e+00</td>\n",
       "      <td>-2.480089e+00</td>\n",
       "      <td>-2.322744e+00</td>\n",
       "      <td>-6.077522e-01</td>\n",
       "      <td>-1.215785e+00</td>\n",
       "      <td>-4.567425e-01</td>\n",
       "      <td>-3.305377e+00</td>\n",
       "      <td>-2.267132e+00</td>\n",
       "      <td>-1.989900e+00</td>\n",
       "      <td>-5.539335e-01</td>\n",
       "      <td>-1.007332e+00</td>\n",
       "      <td>-1.793875e+00</td>\n",
       "      <td>-7.253702e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-5.648886e-01</td>\n",
       "      <td>-6.785716e-01</td>\n",
       "      <td>-4.520777e-01</td>\n",
       "      <td>-6.077522e-01</td>\n",
       "      <td>-1.215785e+00</td>\n",
       "      <td>-4.567425e-01</td>\n",
       "      <td>3.713907e-02</td>\n",
       "      <td>-2.578570e-01</td>\n",
       "      <td>-1.071484e+00</td>\n",
       "      <td>-5.539335e-01</td>\n",
       "      <td>-1.007332e+00</td>\n",
       "      <td>-3.757920e-01</td>\n",
       "      <td>-7.253702e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.019285e-01</td>\n",
       "      <td>2.221872e-01</td>\n",
       "      <td>-4.520777e-01</td>\n",
       "      <td>-6.077522e-01</td>\n",
       "      <td>-4.676098e-02</td>\n",
       "      <td>-4.567425e-01</td>\n",
       "      <td>3.713907e-02</td>\n",
       "      <td>-2.578570e-01</td>\n",
       "      <td>-1.530692e-01</td>\n",
       "      <td>-5.539335e-01</td>\n",
       "      <td>-2.223981e-01</td>\n",
       "      <td>3.332495e-01</td>\n",
       "      <td>-2.172300e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.687457e-01</td>\n",
       "      <td>1.122946e+00</td>\n",
       "      <td>4.832555e-01</td>\n",
       "      <td>9.243963e-01</td>\n",
       "      <td>-4.676098e-02</td>\n",
       "      <td>-4.567425e-01</td>\n",
       "      <td>1.151311e+00</td>\n",
       "      <td>7.467806e-01</td>\n",
       "      <td>7.653460e-01</td>\n",
       "      <td>5.393563e-01</td>\n",
       "      <td>5.625362e-01</td>\n",
       "      <td>1.042291e+00</td>\n",
       "      <td>2.909103e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.036014e+00</td>\n",
       "      <td>1.122946e+00</td>\n",
       "      <td>1.418589e+00</td>\n",
       "      <td>3.988693e+00</td>\n",
       "      <td>2.291288e+00</td>\n",
       "      <td>3.613240e+00</td>\n",
       "      <td>1.151311e+00</td>\n",
       "      <td>1.751418e+00</td>\n",
       "      <td>1.683761e+00</td>\n",
       "      <td>3.819226e+00</td>\n",
       "      <td>2.132405e+00</td>\n",
       "      <td>1.042291e+00</td>\n",
       "      <td>8.802260e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age          Medu          Fedu    traveltime     studytime  \\\n",
       "count  3.000000e+02  3.000000e+02  3.000000e+02  3.000000e+02  3.000000e+02   \n",
       "mean  -1.213844e-15 -3.034610e-17 -6.439294e-17  1.665335e-16 -9.337438e-17   \n",
       "std    1.001671e+00  1.001671e+00  1.001671e+00  1.001671e+00  1.001671e+00   \n",
       "min   -1.331706e+00 -2.480089e+00 -2.322744e+00 -6.077522e-01 -1.215785e+00   \n",
       "25%   -5.648886e-01 -6.785716e-01 -4.520777e-01 -6.077522e-01 -1.215785e+00   \n",
       "50%    2.019285e-01  2.221872e-01 -4.520777e-01 -6.077522e-01 -4.676098e-02   \n",
       "75%    9.687457e-01  1.122946e+00  4.832555e-01  9.243963e-01 -4.676098e-02   \n",
       "max    4.036014e+00  1.122946e+00  1.418589e+00  3.988693e+00  2.291288e+00   \n",
       "\n",
       "           failures        famrel      freetime         goout          Dalc  \\\n",
       "count  3.000000e+02  3.000000e+02  3.000000e+02  3.000000e+02  3.000000e+02   \n",
       "mean  -1.202742e-17 -1.657933e-16 -1.139829e-16  2.294461e-17  2.235249e-16   \n",
       "std    1.001671e+00  1.001671e+00  1.001671e+00  1.001671e+00  1.001671e+00   \n",
       "min   -4.567425e-01 -3.305377e+00 -2.267132e+00 -1.989900e+00 -5.539335e-01   \n",
       "25%   -4.567425e-01  3.713907e-02 -2.578570e-01 -1.071484e+00 -5.539335e-01   \n",
       "50%   -4.567425e-01  3.713907e-02 -2.578570e-01 -1.530692e-01 -5.539335e-01   \n",
       "75%   -4.567425e-01  1.151311e+00  7.467806e-01  7.653460e-01  5.393563e-01   \n",
       "max    3.613240e+00  1.151311e+00  1.751418e+00  1.683761e+00  3.819226e+00   \n",
       "\n",
       "               Walc        health      absences  \n",
       "count  3.000000e+02  3.000000e+02  3.000000e+02  \n",
       "mean   7.179442e-17  2.412885e-16  1.480297e-17  \n",
       "std    1.001671e+00  1.001671e+00  1.001671e+00  \n",
       "min   -1.007332e+00 -1.793875e+00 -7.253702e-01  \n",
       "25%   -1.007332e+00 -3.757920e-01 -7.253702e-01  \n",
       "50%   -2.223981e-01  3.332495e-01 -2.172300e-01  \n",
       "75%    5.625362e-01  1.042291e+00  2.909103e-01  \n",
       "max    2.132405e+00  1.042291e+00  8.802260e+00  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler()  #MinMaxScaler\n",
    "\n",
    "features_to_scale = [ \"age\", \"Medu\", \"Fedu\", \"traveltime\", \"studytime\", \"failures\", \"famrel\", \"freetime\",  \"goout\", \"Dalc\", \"Walc\", \"health\", \"absences\"]\n",
    "#features_to_scale = X_train.columns\n",
    "\n",
    "X_train_scale = X_train.copy()\n",
    "X_test_scale = X_test.copy()\n",
    "X_train_scale[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "X_test_scale[features_to_scale]  = scaler.transform(X_test[features_to_scale]) \n",
    "X_train_scale[features_to_scale].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Models\n",
    "In this section, you will choose 3 supervised learning models that are appropriate for this problem and available in `scikit-learn`. You will first discuss the reasoning behind choosing these three models by considering what you know about the data and each model's strengths and weaknesses. You will then fit the model to varying sizes of training data (100 data points, 200 data points, and 300 data points) and measure the F<sub>1</sub> score. You will need to produce three tables (one for each model) that shows the training set size, training time, prediction time, F<sub>1</sub> score on the training set, and F<sub>1</sub> score on the testing set.\n",
    "\n",
    "**The following supervised learning models are currently available in** [`scikit-learn`](http://scikit-learn.org/stable/supervised_learning.html) **that you may choose from:**\n",
    "- Gaussian Naive Bayes (GaussianNB)\n",
    "- Decision Trees\n",
    "- Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting)\n",
    "- K-Nearest Neighbors (KNeighbors)\n",
    "- Stochastic Gradient Descent (SGDC)\n",
    "- Support Vector Machines (SVM)\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 - Model Application\n",
    "*List three supervised learning models that are appropriate for this problem. For each model chosen*\n",
    "- Describe one real-world application in industry where the model can be applied. *(You may need to do a small bit of research for this  give references!)* \n",
    "- What are the strengths of the model; when does it perform well? \n",
    "- What are the weaknesses of the model; when does it perform poorly?\n",
    "- What makes this model a good candidate for the problem, given what you know about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: **\n",
    "\n",
    "Based on the problem setting, I choose the classification methods based on these following considerations: \n",
    "- there are both categorical and numerical features\n",
    "- the training data size is small, but there are many features.\n",
    "- the features are not necessarily linear. \n",
    "\n",
    "Based on these considerations, I can quickly rule out some of the methods and the reasons are following:\n",
    "\n",
    "- Gaussian Naive Bayes (GaussianNB) generate some statistical probabilities based on the training data and use them as the prior knowledge for inference. It will generalize poorly when there are situations which GaussianNB hasn't seen before in the training data. Given that there are over 40 features but only 300 training data points, I expect GaussianNB is likely to run into such problems. \n",
    "\n",
    "- K-nearest neighbors (KNN) is a beautiful and straightford classification method, and since we are sloving a binary classification problem, basically we know we can set K equals to 2. However, KNN is essentially a **Lazy learner**, and it doesn't learn during training time. In this problem, we want to develop an algorithm which can generalize well and provides insights for future students, therefore KNN is not a good choice.  \n",
    "\n",
    "- Stochastic Gradient Descent (SGDC) has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. It can be very efficient when dealing with large number (>10^5) of sparse training examples. However, although it's an efficient algorithm, SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations, and is sensitive to feature scaling. Given that the size of the data is small and the danger of overfitting, SGD is probably an overkill and the tuning of hyperparameters are likely to complicates things unnecessarily in this case.\n",
    "\n",
    "We are left with choices from Decision Trees, Ensemble Methods, Support Vector Machines (SVM) and Logistic Regression. Since Ensemble Methods are built upon decision tree and are more robustness compared to decision tree, I therefore choose Ensemble Methods, Support Vector Machines (SVM) and Logistic Regression. Below is a discussion on the strength/weakness/industrial applications of these three algorithms.\n",
    "\n",
    "\n",
    "*** Logistic Regression ***\n",
    "\n",
    "1) real-world application in industry:\n",
    "\n",
    "Logistic regression has been shown useful in the area of:\n",
    " - Geographic Image Processing, for example creating a landslide susceptibility mapping. [1]\n",
    " - Image Segmentation and Categorization. [2]\n",
    " - Widely used in the healthcare industry\n",
    "\n",
    "\n",
    "2) strengths and when it performs well:\n",
    "- Outputs have a nice probabilistic interpretation\n",
    "- Robustness: the independent variables dont have to be normally distributed, or have equal variance\n",
    "- Works well for linearly separatable features\n",
    "- Requires less theoretical assumptions than discriminant analysis\n",
    "- Online learning: One can easily update the model to take in new data the future LR allows new data to quickly incorporate into the model.\n",
    "- Many ways to regularize the model to tolerate some errors and avoid over-fitting\n",
    "\n",
    "3) weakness and when it performs poorly:\n",
    "- LR tends to underperform when there are non-linear decision boundaries\n",
    "- LR requires observations to be independent of one another, but if there are not properly identified, LR provides little predictive value.\n",
    "\n",
    "\n",
    "*** Ensemble Methods, specifically Gradient Boosting ***\n",
    "\n",
    "1) real-world application in industry:\n",
    "- GB is an ensemble learning method for improvingthe predictive performance of classification or regression procedures, such as decision trees. Gradient boosting has been shown useful in various Binary classification, Multiclass classification and Regression tasks. \n",
    "- In fact, a numerical of winning solutions for Kaggle's competitions are somewhat related to a popular Gradient Boosting algorithm called XGBoost.  For example, one industrial application is the recommendation system for ecommerce and many websites, and GB is the winning algorithm in a recent competition on collaborative filtering (CF). [3] \n",
    "\n",
    "\n",
    "2) strengths and when it performs well: \n",
    "- GB models are simple to understand and interpret like decision trees. People are able to understand decision tree models after a brief explanation.\n",
    "- GB models can automatically select variables, \n",
    "- GB models are robust to outliers, missing data and numerous correlated and irrelevant variables,\n",
    "- GB can construct variable importance in exactly the same way as random forest.\n",
    "- Some of the successful GB algorithms such as XGBoost are sparse-aware and can effectively handle data sparsity.\n",
    "\n",
    "3) weakness and when it performs poorly: \n",
    "- while GB has many advantages especially in handingly tabulated datasets, it is a lot less famous in the area of computer vision and speech/language applications, where convolutional neural networks and recurrent neural networks have demonstrated great success. \n",
    "- GB may experience the same issue as decision tree in terms of local maxima. \n",
    "\n",
    "\n",
    "*** SVM ***\n",
    "\n",
    "1) real-world application in industry: [4]\n",
    "- text and hypertext categorization\n",
    "- Classification of images\n",
    "- Hand-written characters can be recognized using SVM\n",
    "- The SVM algorithm has been widely applied in the biological and other sciences. \n",
    "\n",
    "\n",
    "2) strengths and when it performs well:\n",
    "- Kernel trick: efficient for dealing with high dimensional feature space. One can build in expert knowledge about the problem via engineering the kernel. \n",
    "- SVM is defined by a convex optimisation problem (no local minima) for which there are efficient methods (dual representation). \n",
    "- SVM have regularization parameters to tolerate some errors and avoid over-fitting\n",
    "- Provides a good out-of-sample generalization, if the parameters C and gamma are appropriate chosen. In other words, SVM might be more robust even when the training sample has some bias\n",
    "\n",
    "3) weakness and when it performs poorly:\n",
    "- Bad interpretability: SVMs are black boxes\n",
    "- High computational cost: SVMs scale exponentially in training time\n",
    "- Users might need to have certain domain knowledge to use kernel function\n",
    "\n",
    "\n",
    "\n",
    "references\n",
    "\n",
    "[1] doi.org/10.1016/j.geomorph.2004.06.010\n",
    "\n",
    "[2] http://ttic.uchicago.edu/~xren/publication/xren_iccv03_discrim.pdf\n",
    "\n",
    "[3] Maksims Volkovs, Guang Wei Yu, and Tomi Poutanen. 2017. Content-based Neighbor Models for Cold Start in Recommender Systems. In Proceedings of RecSys Challenge 17, Como, Italy, August 27, 2017, 6 pages. https://doi.org/10.1145/3124791.3124792\n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/Support_vector_machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Run the code cell below to initialize three helper functions which you can use for training and testing the three supervised learning models you've chosen above. The functions are as follows:\n",
    "- `train_classifier` - takes as input a classifier and training data and fits the classifier to the data.\n",
    "- `predict_labels` - takes as input a fit classifier, features, and a target labeling and makes predictions using the F<sub>1</sub> score.\n",
    "- `train_predict` - takes as input a classifier, and the training and testing data, and performs `train_clasifier` and `predict_labels`.\n",
    " - This function will report the F<sub>1</sub> score for both the training and testing data separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Trained model in {:.4f} seconds\".format(end - start))\n",
    "\n",
    "    \n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    # Print and return results\n",
    "    print(\"Made predictions in {:.4f} seconds.\".format(end - start))\n",
    "    return f1_score(target.values, y_pred, pos_label='yes')\n",
    "\n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    # Indicate the classifier and the training set size\n",
    "    print(\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    print(\"F1 score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "    print(\"F1 score for test set: {:.4f}.\".format(predict_labels(clf, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Model Performance Metrics\n",
    "With the predefined functions above, you will now import the three supervised learning models of your choice and run the `train_predict` function for each one. Remember that you will need to train and predict on each classifier for three different training set sizes: 100, 200, and 300. Hence, you should expect to have 9 different outputs below  3 for each model using the varying training set sizes. In the following code cell, you will need to implement the following:\n",
    "- Import the three supervised learning models you've discussed in the previous section.\n",
    "- Initialize the three models and store them in `clf_A`, `clf_B`, and `clf_C`.\n",
    " - Use a `random_state` for each model you use, if provided.\n",
    " - **Note:** Use the default settings for each model  you will tune one specific model in a later section.\n",
    "- Create the different training set sizes to be used to train each model.\n",
    " - *Do not reshuffle and resplit the data! The new training points should be drawn from `X_train` and `y_train`.*\n",
    "- Fit each model with each training set size and make predictions on the test set (9 in total).  \n",
    "**Note:** Three tables are provided after the following code cell which can be used to store your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a LogisticRegression using a training set size of 100. . .\n",
      "Trained model in 0.0030 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for training set: 0.8372.\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.6870.\n",
      "Training a LogisticRegression using a training set size of 200. . .\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for training set: 0.8390.\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for test set: 0.6970.\n",
      "Training a LogisticRegression using a training set size of 300. . .\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for training set: 0.8424.\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for test set: 0.7681.\n",
      "Training a GradientBoostingClassifier using a training set size of 100. . .\n",
      "Trained model in 0.0550 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for training set: 1.0000.\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.7299.\n",
      "Training a GradientBoostingClassifier using a training set size of 200. . .\n",
      "Trained model in 0.0701 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for training set: 0.9845.\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.7704.\n",
      "Training a GradientBoostingClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0961 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for training set: 0.9684.\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for test set: 0.7647.\n",
      "Training a SVC using a training set size of 100. . .\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for training set: 0.8258.\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.8176.\n",
      "Training a SVC using a training set size of 200. . .\n",
      "Trained model in 0.0030 seconds\n",
      "Made predictions in 0.0020 seconds.\n",
      "F1 score for training set: 0.8200.\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.8133.\n",
      "Training a SVC using a training set size of 300. . .\n",
      "Trained model in 0.0060 seconds\n",
      "Made predictions in 0.0040 seconds.\n",
      "F1 score for training set: 0.8491.\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.8322.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import the three supervised learning models from sklearn\n",
    "\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# TODO: Initialize the three models\n",
    "#clf_A = GaussianNB()\n",
    "#clf_B = RandomForestClassifier()\n",
    "clf_A = LogisticRegression()\n",
    "clf_B = GradientBoostingClassifier()\n",
    "clf_C = SVC(kernel='rbf')\n",
    "\n",
    "# TODO: Set up the training set sizes\n",
    "X_train_100 = X_train_scale[:100]\n",
    "y_train_100 = y_train[:100]\n",
    "\n",
    "X_train_200 = X_train_scale[:200]\n",
    "y_train_200 = y_train[:200]\n",
    "\n",
    "X_train_300 = X_train_scale[:300]\n",
    "y_train_300 = y_train[:300]\n",
    "\n",
    "# TODO: Execute the 'train_predict' function for each classifier and each training set size\n",
    "train_predict(clf_A, X_train_100, y_train_100, X_test_scale, y_test)\n",
    "train_predict(clf_A, X_train_200, y_train_200, X_test_scale, y_test)\n",
    "train_predict(clf_A, X_train_300, y_train_300, X_test_scale, y_test)\n",
    "\n",
    "train_predict(clf_B, X_train_100, y_train_100, X_test_scale, y_test)\n",
    "train_predict(clf_B, X_train_200, y_train_200, X_test_scale, y_test)\n",
    "train_predict(clf_B, X_train_300, y_train_300, X_test_scale, y_test)\n",
    "\n",
    "train_predict(clf_C, X_train_100, y_train_100, X_test_scale, y_test)\n",
    "train_predict(clf_C, X_train_200, y_train_200, X_test_scale, y_test)\n",
    "train_predict(clf_C, X_train_300, y_train_300, X_test_scale, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Results\n",
    "Edit the cell below to see how a table can be designed in [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#tables). You can record your results from above in the tables provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Classifer 1 - Logistic Regression**  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | F1 Score (train) | F1 Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 100               | 0.0030                  |   0.0000               | 0.8372           |    0.6870       |\n",
    "| 200               | 0.0010                  |   0.0000               | 0.8390           |    0.6970       |\n",
    "| 300               | 0.0010                  |   0.0000               | 0.8424           |    0.7681       |\n",
    "\n",
    "** Classifer 2 - Gradient Boosting Classifier**  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | F1 Score (train) | F1 Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 100               |   0.0550                | 0.0010                 |    1.0000        |  0.7299         |\n",
    "| 200               |     0.0701              |  0.0010                |   0.9845         |  0.7704         |\n",
    "| 300               |    0.0961               |  0.0010                |    0.9684        |      0.7647     |\n",
    "\n",
    "** Classifer 3 - Support Vector Machines (SVM)**  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | F1 Score (train) | F1 Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 100               |   0.0010                |    0.0000              |   0.8258         | 0.8176          |\n",
    "| 200               |   0.0030                |    0.0020              |   0.8200         |  0.8133         |\n",
    "| 300               |   0.0060                |    0.0040              |   0.8491         |  0.8322         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Best Model\n",
    "In this final section, you will choose from the three supervised learning models the *best* model to use on the student data. You will then perform a grid search optimization for the model over the entire training set (`X_train` and `y_train`) by tuning at least one parameter to improve upon the untuned model's F<sub>1</sub> score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 - Choosing the Best Model\n",
    "*Based on the experiments you performed earlier, in one to two paragraphs, explain to the board of supervisors what single model you chose as the best model. Which model is generally the most appropriate based on the available data, limited resources, cost, and performance?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: **\n",
    "\n",
    "Based on the training/test time, as well as the test F1 score, I choose SVM as the best model for the data for the following reasons. Given that logistic regression is essentially a linear model, and there is no guarantee that the features are linearly separatable, it's not surprising that SVM with a non-linear kernel may better represent the model better than logistic regression. As to the gradient boosting classifier, it looks like the untuned model has achieved a very high training F1 score but performs much worse during test time. As gradient boosting classifier can easily lead to overfitting, I interpret the huge training and testing F1 score different as largely due to overfitting and the small training sample size.  \n",
    "\n",
    "In terms of the training/testing time, gradient boosting is the most computationally costly, while LR and SVM are computationally fast on roughly the same scale. Although LR is slightly faster, both LR and SVM are rather fast given the data size.\n",
    "\n",
    "In terms of the available data, I wouldn't expect the data size to increase dramatically in the future, since the school probably evaluates students only a few times a year so the available data size won't increase dramatically. As gradient boosting seems to overfit and may require more data (or regularization), I think SVM is a good choice again given the size of the availabel data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 - Model in Layman's Terms\n",
    "*In one to two paragraphs, explain to the board of directors in layman's terms how the final model chosen is supposed to work. Be sure that you are describing the major qualities of the model, such as how the model is trained and how the model makes a prediction. Avoid using advanced mathematical or technical jargon, such as describing equations or discussing the algorithm implementation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: **\n",
    "\n",
    "\n",
    "\n",
    "Support Vector Machine (SVM) is a type of linear separator. Suppose we want to split green circles from the blue ones above by drawing a line. Notice that there are an infinite number of lines that will accomplish this task. SVMs, in particular, find the **\"maximum-margin\"** line - this is the line \"in the middle\". In other words, SVM finds a line which has the largest overall distance (called \"margin\") to the points on each size. Intuitively, the max margin works well because it has the largest tolerance for noise and mistakes on either side.  \n",
    "![title](max_margin.png)\n",
    "Note that the example illustrated here is shown in a 2D plane, and in real cases the dots are located in a high dimensional space, so the max-margin line we draw here becomes a hyper plane, but you get the idea.\n",
    "\n",
    "**Support vectors**: Once we understand the concept of maximizing the margin, we can immediately see that the margin lines are only determined by a subset of dots that are close to the margin lines, while the data points that are away from the margin lines have no influence on the outcome whatsoever. This subset of points are called \"Support vectors\".  \n",
    "![title](support_vectors.png)\n",
    "\n",
    "Note that SVM is versatile and can be used for **both classification and regression**. When we want to separate the dots for classification, we want to maximize the margin. But when we want to predict a trend line following the dots (regression), we want to find the minimum distance between two margin lines which include as many data points in between as possible. \n",
    "\n",
    "\n",
    "**Hard and soft margin**: We may notice from the above figures that if we were to require a perfectly separation between the green and blue dots (we call this \"hard margin\"), the margin lines will be very sensitive to any noisy points that lie in between, and in some cases it's even not possible to perfectly separate the green and blue points.  However, if we allow some errors/misclassifications in how the margin lines separate the dots, we can avoid the case where a single outlier totally distors the margin lines (we call this \"soft margin\"). By using a soft margin, we can essentially avoid overfitting to noisy points and achieve a more generalized model to fit the unseen data. \n",
    "\n",
    "**Kernel trick**. We may come across a problem where the datapoints are not linearly separatable in their original space, for example in the case shown below. [1] If we project the data points from their original space to a higher dimension that represents the square of distance of each point to the origin, we can see that the data points are totally separatable as their distance to the origin are dinstinctively different. Simply put, the way we transform the original feature space to a higher dimensional space is call \"kernel trick\". It adds non-linearity to the feature space based on our domain knowledge, and allows SVM to classify data points that are not linearly separatable.  \n",
    "\n",
    "\n",
    "![title](kernel_trick.png)\n",
    "[1] figure source: http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Model Tuning\n",
    "Fine tune the chosen model. Use grid search (`GridSearchCV`) with at least one important parameter tuned with at least 3 different values. You will need to use the entire training set for this. In the code cell below, you will need to implement the following:\n",
    "- Import [`sklearn.grid_search.GridSearchCV`](http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.GridSearchCV.html) and [`sklearn.metrics.make_scorer`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html).\n",
    "- Create a dictionary of parameters you wish to tune for the chosen model.\n",
    " - Example: `parameters = {'parameter' : [list of values]}`.\n",
    "- Initialize the classifier you've chosen and store it in `clf`.\n",
    "- Create the F<sub>1</sub> scoring function using `make_scorer` and store it in `f1_scorer`.\n",
    " - Set the `pos_label` parameter to the correct value!\n",
    "- Perform grid search on the classifier `clf` using `f1_scorer` as the scoring method, and store it in `grid_obj`.\n",
    "- Fit the grid search object to the training data (`X_train`, `y_train`), and store it in `grid_obj`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=8, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=2, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Made predictions in 0.0040 seconds.\n",
      "Tuned model has a training F1 score of 0.8203.\n",
      "Made predictions in 0.0020 seconds.\n",
      "Tuned model has a testing F1 score of 0.8344.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.grid_search import GridSearchCV \n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "\n",
    "parameters = {\n",
    "                \"kernel\": ['rbf', 'poly'],\n",
    "                \"degree\": [2,3,4],\n",
    "                \"C\": [1, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20],\n",
    "                \"gamma\": [ 0.0008, 0.0009, 0.001, 0.0011, 0.0012, ],\n",
    "                \n",
    "            }\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = SVC( random_state = 0)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=f1_scorer, n_jobs = 4,)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj.fit(X_train_scale,y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "print(clf)\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print(\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train_scale, y_train)))\n",
    "print(\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test_scale, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 - Final F<sub>1</sub> Score\n",
    "*What is the final model's F<sub>1</sub> score for training and testing? How does that score compare to the untuned model?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: **\n",
    "\n",
    "The final model's F1 score for training and testing are 0.8203 and 0.8344, respectively. Compared to the untuned model, and training F1 score is decreased from 0.8491 and the testing F1 score is slightly increased from 0.8322. \n",
    "\n",
    "The decrease in training score is not surprising since the tuned model has a regularization term, which adds additional losses to the overall loss function and increased bias during training. This however helps to regularize the model to avoid overfitting, and the tuned model therefore has a reduced variance and increased F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \n",
    "**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
